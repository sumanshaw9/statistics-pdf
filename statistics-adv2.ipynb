{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7783ca2d-9245-429e-9057-a2e543b8a602",
   "metadata": {},
   "source": [
    "Question1: Define the z-statistic and explain its relationship to the standard normal distribution. How is the\r\n",
    "z-statistic used in hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34001ade-f767-4c56-9e1d-37c694f3950f",
   "metadata": {},
   "source": [
    "Sol:\n",
    "The z-statistic, also known as the z-score, is a measure that indicates how many standard deviations an individual data point is from the mean of a dataset. It is calculated using the formula: z= (X‚àíŒº)/œÉ\n",
    "Where:\n",
    "X is the value of the data point,\n",
    "Œº is the mean of the population,\n",
    "œÉ is the standard deviation of the population.\n",
    "\n",
    "##### Relationship to the Standard Normal Distribution\n",
    "The z-statistic transforms data from any normal distribution into the standard normal distribution, which has:\n",
    "- A mean of 0, and\n",
    "- A standard deviation of 1.\n",
    "This transformation allows comparisons across different distributions by standardizing them. A z-score of:\n",
    "- 0 means the data point is exactly at the mean,\n",
    "- Positive values mean the data point is above the mean,\n",
    "- Negative values mean the data point is below the mean.\n",
    "- ##### Z-Statistic in Hypothesis Testing\n",
    "The z-statistic is widely used in hypothesis testing, especially for testing population means and proportions when the population standard deviation is known or the sample size is large (usually n>30).\n",
    "\n",
    "###### Steps in Hypothesis Testing:\n",
    "1. State the hypotheses:\n",
    "\n",
    "- Null hypothesis (H 0): Assumes no effect or no difference (e.g., Œº=Œº 0).\n",
    "- Alternative hypothesis (H A): Assumes there is an effect or difference (e.g., ùúá‚â†ùúá0).\n",
    "\n",
    "2. Compute the z-statistic: The z-statistic for a sample mean is calculated as: $$ z = \\frac{\\bar{X} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} $$\n",
    "Where:\n",
    "$\\bar{X}$ is the sample mean,\n",
    "\n",
    "${\\mu_0}$ is the hypothesized population mean,\n",
    "\n",
    "œÉ is the population standard deviation,\n",
    "\n",
    "n is the sample size.\n",
    "\n",
    "3. Compare to critical values: Based on the chosen significance level (Œ±), the z-statistic is compared against critical values from the standard normal distribution:\n",
    "\n",
    "- For a two-tailed test, reject ${H_0}$\n",
    " if ‚à£ùëß‚à£ exceeds the critical value (e.g., ¬±1.96 for ùõº=0.05).\n",
    "- For a one-tailed test, reject ${H_0}$ if the z-statistic is either too high or too low compared to the critical value.\n",
    "\n",
    "4. Make a decision:\n",
    "- If the z-statistic falls in the rejection region, the null hypothesis is rejected in favor of the alternative hypothesis.\n",
    "- Otherwise, the null hypothesis is not rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a81437-6ba3-41d2-a592-8ac236e237c1",
   "metadata": {},
   "source": [
    "Question2 : What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is\r\n",
    "very small (e.g., 0.01)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642084d-f391-4785-8906-180ac65608f7",
   "metadata": {},
   "source": [
    "A *p-value* is a probability that helps determine the significance of results in hypothesis testing. It measures the likelihood of observing the test statistic, or more extreme values, assuming that the null hypothesis is true.\n",
    "\n",
    "Role of p-value in hypothesis testing:\n",
    "\n",
    "1. Hypothesis testing begins with formulating two hypotheses:\n",
    "- Null hypothesis (${H_0}$): There is no effect or difference.\n",
    "- Alternative hypothesis (${H_1}$): There is an effect or difference.\n",
    "\n",
    "2. After collecting data and conducting a statistical test, a p-value is calculated to assess whether the observed data provide enough evidence to reject the null hypothesis.\n",
    "\n",
    "3. The p-value is compared to a predefined significance level (Œ±), commonly set at 0.05 (or 5%):\n",
    "- If the p-value is less than Œ±: The null hypothesis is rejected, indicating strong evidence against it, and supporting the alternative hypothesis.\n",
    "- If the p-value is greater than Œ±: There is insufficient evidence to reject the null hypothesis.\n",
    "\n",
    "##### Interpreting a very small p-value (e.g., 0.01):\n",
    "- A p-value of 0.01 indicates that there is only a 1% probability of observing the test statistic, or something more extreme, if the null hypothesis is true.\n",
    "- Conclusion: The smaller the p-value, the stronger the evidence against the null hypothesis. In this case, if Œ±=0.05, a p-value of 0.01 would lead to the rejection of the null hypothesis. It suggests that the observed results are highly unlikely under the assumption of no effect or difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e389c1-27c7-4a36-8977-2092e3ff66c2",
   "metadata": {},
   "source": [
    "Question3: Compare and contrast the binomial and Bernoulli distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5c4b0-2646-4aab-a672-1cb203bb7b6c",
   "metadata": {},
   "source": [
    "Sol:\n",
    "The Binomial and Bernoulli distributions are both important probability distributions in statistics, especially for modeling binary or \"success-failure\" type experiments. Although related, they have distinct characteristics. Here's a comparison:\n",
    "\n",
    "1. Definition:\n",
    "- Bernoulli Distribution: Represents the outcome of a single trial that can have two possible outcomes: \"success\" (with probability ùëù) and \"failure\" (with probability 1‚àíùëù). It‚Äôs the simplest discrete distribution.\n",
    "- Binomial Distribution: Represents the number of successes in a fixed number (ùëõ) of independent Bernoulli trials, each with the same probability of success (ùëù).\n",
    "2. Parameters:\n",
    "- Bernoulli Distribution:\n",
    "  - ùëù: The probability of success (0 ‚â§ ùëù ‚â§ 1).\n",
    "- Binomial Distribution:\n",
    "  - n: The number of trials.\n",
    "  - p: The probability of success in each trial (0 ‚â§ p ‚â§ 1).\n",
    "3. Support (Possible Values):\n",
    "- Bernoulli Distribution: The outcome can only be either 0 (failure) or 1 (success). It‚Äôs used for a single trial.\n",
    "  - ùëã‚àà{0,1}\n",
    "- Binomial Distribution: The outcome is the number of successes in n trials, ranging from 0 (no successes) to n (all trials successful).\n",
    "  - X‚àà{0,1,2,‚Ä¶,n}\n",
    "4. Probability Mass Function (PMF):\n",
    "- Bernoulli PMF: P(X = x) = p^x * (1 - p)^(1 - x), where x ‚àà {0, 1}\n",
    "  Where p is the probability of success.\n",
    "- Binomial PMF: P(X = k) = (n choose k) * p^k * (1 - p)^(n - k), where k ‚àà {0, 1, 2, ..., n}\n",
    "  Where n is the number of trials, k is the number of successes, and p is the probability of success.\n",
    "5. Mean and Variance:\n",
    "- Bernoulli:\n",
    "  - Mean: E(X)=p\n",
    "  - Variance: Var(X)=p(1‚àíp)\n",
    "- Binomial:\n",
    "  - Mean: E(X)=np\n",
    "  - Variance: Var(X)=np(1‚àíp)\n",
    "6. Use Case:\n",
    "- Bernoulli: Used to model the outcome of a single event, such as flipping a coin once or determining if a product is defective (yes/no).\n",
    "- Binomial: Used to model the number of successes in multiple independent trials of a Bernoulli process, such as flipping a coin 10 times or counting      how many of 100 products are defective.\n",
    "7. Relationship Between Binomial and Bernoulli:\n",
    "- Bernoulli is a special case of the Binomial distribution where n=1. In other words, if you perform a single trial of a binomial experiment, you get a - Bernoulli distribution.\n",
    "8. Examples:\n",
    "- Bernoulli Example: Suppose we flip a fair coin once, where heads is considered a success (p=0.5). The probability of getting a head (1) or a tail (0)    follows a Bernoulli distribution.\n",
    "- Binomial Example: Suppose we flip a fair coin 5 times. The number of heads (successes) follows a Binomial distribution with parameters n=5 and  p=0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02609fc8-5ac9-4aa6-b89a-cfa6b06210ad",
   "metadata": {},
   "source": [
    "Question 4: Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli\n",
    "distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c496b-89ca-4a40-b899-4f27a73848a3",
   "metadata": {},
   "source": [
    "The binomial distribution is used under specific conditions and is closely related to the Bernoulli distribution. Here‚Äôs a breakdown of when to use the binomial distribution and its connection to the Bernoulli distribution:\n",
    "\n",
    "##### Conditions for Using the Binomial Distribution\n",
    "1. Fixed Number of Trials (n): The experiment consists of a predetermined number of trials or observations. This number must remain constant.\n",
    "\n",
    "2. Two Possible Outcomes: Each trial results in one of two outcomes, often referred to as \"success\" (e.g., a win, a heads in a coin toss) or \"failure\" (e.g., a loss, tails).\n",
    "\n",
    "3. Constant Probability (p): The probability of success (denoted as \n",
    "ùëù\n",
    "p) is the same for each trial. Similarly, the probability of failure is \n",
    "1\n",
    "‚àí\n",
    "ùëù\n",
    "1‚àíp.\n",
    "\n",
    "4. Independent Trials: The trials are independent, meaning the outcome of one trial does not affect the outcomes of other trials.\n",
    "\n",
    "##### Relation to the Bernoulli Distribution\n",
    "- The Bernoulli distribution is a special case of the binomial distribution where the number of trials n=1. It describes a single trial with two           possible outcomes, typically encoded as 1 (success) and 0 (failure).\n",
    "\n",
    "- The binomial distribution can be seen as the sum of multiple independent Bernoulli trials. Specifically, if you conduct n independent Bernoulli trials   (each with probability \n",
    "  p of success), the total number of successes follows a binomial distribution with parameters n and p.\n",
    "\n",
    "##### Mathematical Representation\n",
    "- Bernoulli Distribution: P(X = k) = p^k * (1 - p)^(1 - k) ,where k ‚àà {0, 1} \n",
    "\n",
    "- Binomial Distribution:  P(X = k) = (n choose k) * p^k * (1 - p)^(n - k), where k ‚àà {0, 1, 2, ..., n}\n",
    "  where (n choose k) is the binomial coefficient, representing the number of ways to choose k successes from n trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752a6fa-2410-4c7e-a6a7-c29708dcb8ef",
   "metadata": {},
   "source": [
    "Question5: What are the key properties of the Poisson distribution, and when is it appropriate to use this \n",
    "distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540c718-337a-4aa8-896f-48adceff76ba",
   "metadata": {},
   "source": [
    "The Poisson distribution is a discrete probability distribution that models the number of events occurring within a fixed interval of time or space. Here are its key properties and the conditions under which it is appropriate to use this distribution:\n",
    "\n",
    "##### Key Properties of the Poisson Distribution\n",
    "1. Discrete Nature: The Poisson distribution is used for count data, where the variable of interest is the number of occurrences of an event within a specified interval.\n",
    "\n",
    "2. Parameter (Œª): The distribution is characterized by a single parameter Œª (lambda), which represents the average number of events in the given interval. Both the mean and variance of the Poisson distribution equal Œª.\n",
    "\n",
    "3. Probability Mass Function (PMF): The probability of observing k events in an interval is given by the formula\n",
    "   P(X = k) = (e^(-Œª) * Œª^k) / k!, for k = 0, 1, 2, ...\r",
    "    where e is the base of the natural logarithm, and\n",
    "k! is the factorial of\n",
    "k.\n",
    "\n",
    "4. Memoryless Property: The Poisson process has a memoryless property, meaning that the number of events occurring in non-overlapping intervals is independent of each other.\n",
    "\n",
    "5. Additivity: If you have two independent Poisson processes with rates\n",
    "\n",
    "  andŒª\n",
    " , the sum of these processes is also a Poisson process with a rate ofŒª1 + Œª2.\n",
    "\n",
    "##### When to Use the Poisson Distribution\n",
    "\n",
    "The Poisson distribution is appropriate under the following conditions:\n",
    "1. Rare Events: It is commonly used to model the number of rare events occurring in a fixed interval (time, area, volume, etc.). For example, it can be used to model the number of phone calls received at a call center in an hour.\n",
    "\n",
    "2. Fixed Interval: The events are counted in a defined interval of time or space, such as the number of accidents at a traffic intersection per month.\n",
    "\n",
    "3. Independence of Events: The events should be independent; the occurrence of one event should not influence the occurrence of another.\n",
    "\n",
    "4. Constant Rate: The average rate Œª of occurrence should be constant over the interval. For instance, if you are counting the number of emails received per hour, you assume a steady rate rather than a fluctuating one.\n",
    "\n",
    "5. No Simultaneous Events: The probability of two or more events occurring at exactly the same instant is negligible.\n",
    "\n",
    "##### Examples of Poisson Distribution Applications\n",
    "- The number of decay events per unit time from a radioactive source.\n",
    "- The number of arrivals at a service point, such as a bank or a hospital.\n",
    "- The number of customer purchases during a promotional period in a store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170bab0f-499a-4559-814b-f3d1c66ccba6",
   "metadata": {},
   "source": [
    "Question6: Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a \n",
    "PDF differ from a probability mass function (PMF)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbbc4a-49e3-4c4c-a2cb-609ac9a9e0ef",
   "metadata": {},
   "source": [
    "##### Probability Distribution\n",
    "A probability distribution is a mathematical function that describes the likelihood of different outcomes in a random variable. It provides a way to map all possible values of a random variable to their associated probabilities. Probability distributions can be classified into two main types:\n",
    "\n",
    "1. Discrete Probability Distribution: Used for discrete random variables, where the variable can take on a countable number of values (e.g., the number of heads in coin flips).\n",
    "2. Continuous Probability Distribution: Used for continuous random variables, where the variable can take on any value within a given range (e.g., the height of individuals).\n",
    "##### Probability Density Function (PDF)\n",
    "A probability density function (PDF) is a specific type of probability distribution used for continuous random variables. The PDF describes the relative likelihood of the random variable taking on a particular value. The key properties of a PDF are:\n",
    "\n",
    "1. Non-Negative: The value of the PDF is always non-negative.\n",
    "2. Area Under the Curve: The total area under the PDF curve over its entire range is equal to 1, representing the total probability.\n",
    "3. Probability Calculation: The probability that a continuous random variable falls within a certain interval is given by the area under the PDF curve over that interval.\n",
    "s##### Differences Between PDF and PMF\n",
    "The main differences between a probability density function (PDF) and a probability mass function (PMF) are:\n",
    "| Feature                     | Probability Mass Function (PMF)                      | Probability Density Function (PDF)               |\r\n",
    "|-----------------------------|------------------------------------------------------|-------------------------------------------------|\r\n",
    "| **Type of Random Variable**  | Used for discrete random variables                   | Used for continuous random variables             |\r\n",
    "| **Definition**              | Gives the probability that a discrete random variable equals a specific value | Gives the density of probabilities over a range of values |\r\n",
    "| **Probability Values**      | Directly provides probabilities for specific outcomes | Does not give probabilities directly; must integrate to find probabilities over intervals |\r\n",
    "| **Sum vs. Area**           | The sum of all probabilities in a PMF equals 1     | The total area under the PDF curve equals 1    |\r\n",
    "| **Notation**                | \\( P(X = k) \\) for \\( k \\) in discrete values       | \\( f(x) \\) for \\( x \\) in continuous value    |\r\n",
    "    |\r\n",
    "    |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb945973-b64b-42e1-b73d-16d276a2f0e3",
   "metadata": {},
   "source": [
    "Question7: Explain the Central Limit Theorem (CLT) with example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46581f-c93f-4470-a4b8-a39275d272be",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental statistical principle that states that the distribution of the sample mean (or sum) of a sufficiently large number of independent and identically distributed random variables will approximate a normal distribution, regardless of the original distribution of the variables. This theorem is significant because it allows statisticians to make inferences about population parameters based on sample statistics.\n",
    "\n",
    "##### Key Points of the Central Limit Theorem\n",
    "1. Independence: The sampled observations must be independent of each other.\n",
    "2. Identical Distribution: The random variables should be identically distributed, meaning they have the same probability distribution.\n",
    "3. Sample Size: As the sample size n increases (typically n‚â•30 is considered sufficient), the sampling distribution of the sample mean approaches a         normal distribution, even if the original population distribution is not normal.\n",
    "4. Mean and Variance: If the original population has a mean Œº and a standard deviation œÉ, the sampling distribution of the sample mean will have:\n",
    "- Mean: Œº_XÃÑ = ŒºŒº- \n",
    "Standard deviation (Standard Error):œÉ_XÃÑ = œÉ / ‚àön\n",
    "‚Äã\n",
    "#####  \n",
    "Example of the Central Limit Theorem\n",
    "Let's illustrate the CLT with a simple example:\n",
    "\n",
    "Scenario\n",
    "Suppose we have a population of exam scores for a class of students that is uniformly distributed between 0 and 100. The uniform distribution means that every score within this range is equally likely. We want to understand the distribution of the average scores from different samples taken from this population.\n",
    "\n",
    "##### Step-by-Step Illustration\n",
    "1. Population Distribution- \n",
    "The population of scores is uniformly distributed, so its me 0\n",
    "Œº=50 and standard deviatio7\n",
    "œÉ‚âà28.87.\n",
    "2. Taking Samples- \n",
    "We take multiple random samples of siz0\n",
    "n=30 from this population.\n",
    "\n",
    "3. Calculating Sample Means- \n",
    "For each sample, calculate the mean score. After repeating this process (say, 1000 times), we obtain a distribution of sample mean\n",
    "  \n",
    "4. Resulting Distribution-\n",
    "According to the CLT, even though the original population distribution (exam scores) is uniform and not normal, the distribution of the sampl          e mea s will approximate a normal distribution as the number of samples increase\n",
    "    \n",
    "5. Visualizing the Result  - \n",
    "If you plot the histogram of the sample means, you will see that it approaches a normal distribution centered around Œº=50, with reduced variability (spread) compared to the original uniform distribution.\n",
    "\n",
    "##### Importance of the Central Limit Theorem\n",
    "Statistical Inference: The CLT allows us to use normal probability techniques to make inferences about population parameters, even when the original data does not follow a normal distributio.- \n",
    "\n",
    "Hypothesis Testing and Confidence Intervals: The theorem forms the basis for many statistical methods, including hypothesis testing and the construction of confidence intervalon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0a147-bdea-4190-9272-f4af635b30a0",
   "metadata": {},
   "source": [
    "Question8: Compare z-scores and t-scores. When should you use a z-score, and when should a t-score be applied instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adf4e8-b2ed-4ff2-a088-2be464c1c051",
   "metadata": {},
   "source": [
    "sol:\n",
    "Z-scores and t-scores are both standardized scores used in statistics to indicate how many standard deviations an element is from the mean. However, they are used in different contexts and have distinct characteristics. Here‚Äôs a comparison of the two:\n",
    "\n",
    "##### Z-Scores\n",
    "\n",
    "- Definition: A z-score (or standard score) measures the distance of a data point from the mean in terms of standard deviations. It is calculated using the formula: ùëß = (X-ùúá)/œÉ , where X is the value, Œº is the population mean, and œÉ is the population standard deviation.\n",
    "\n",
    "- Population Data: Z-scores are used when you have a sample that is large (typically n‚â•30) or when the population standard deviation (œÉ) is known.\n",
    "\n",
    "- Normal Distribution: Z-scores are applied when the underlying data follows a normal distribution, or when the sample size is large enough for the Central Limit Theorem to apply.\n",
    "\n",
    "- Applications: Commonly used in hypothesis testing, confidence intervals, and when comparing data from different normal distributions.\n",
    "\n",
    "##### T-Scores\n",
    "\n",
    "- Definition: A t-score is similar to a z-score but is used when the sample size is small (typically n<30) and the population standard deviation is        unknown. The formula for calculating a t-score is: t = (X - XÃÑ) / (s / ‚àön) , \n",
    "where\n",
    "X is the value,$\\bar{X}$  is the sample mean,\n",
    "s is the sample standard deviation, and\n",
    "n is the sample size.\n",
    "- \n",
    "Sample Data: T-scores are specifically used when analyzing sample data, particularly when dealing with smaller sample sizes where the variability is greater.\n",
    "- \n",
    "Student's t-Distribution: T-scores follow the Student's t-distribution, which has thicker tails compared to the normal distribution. This accounts for the added uncertainty in estimating the population standard deviation from a small sample.\n",
    "- \n",
    "Applications: Used in hypothesis testing, confidence intervals, and situations where the sample size is small, particularly in estimating population means\n",
    "\n",
    "##### When to Use Z-Scores vs. T-Scores\n",
    "| Criteria                     | Z-Score                             | T-Score                                 |\r\n",
    "|------------------------------|-------------------------------------|-----------------------------------------|\r\n",
    "| **Sample Size**              | Large sample (n ‚â• 30)              | Small sample (n < 30)                   |\r\n",
    "| **Population Standard Deviation** | Known                           | Unknown                                  |\r\n",
    "| **Distribution**             | Normal distribution or large sample | Student's t-distribution                 |\r\n",
    "| **Formula**                  | \\( z = \\frac{X - \\mu}{\\sigma} \\)  | \\( t = \\frac{X - \\bar{X}}{s / \\sqrt{n}} \\) |\r\n",
    "| **Use Case**                 | Hypothesis testing, confidence intervals with known population parameters | Hypothesis testing, confidence intervals with unknown population parameters |\r\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7977a18-5327-4f25-8a7a-46e91658b000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
